{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/FZOSIGZyNp+xIX1/ZaUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shindora/NLP-with-PyTorch/blob/master/Chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUz_FPVpk78a",
        "colab_type": "text"
      },
      "source": [
        "**Corpus,Tokens,Types**<br>\n",
        "- All NLP methods, classic or modern, begin with a text dataset, called **corpus**, it usually contains raw text (in ASCII or UTF-8).\n",
        "- The raw text is a sequence of characters (bytes), but most times it is usefull to group those characters into contiguous units called **tokens**. In English, tokens correspond to words and numeric sequences separated by space or punctuation.\n",
        "- The process of breaking a text down into tokens is called **tokenization**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLCTt8dBmiZ2",
        "colab_type": "code",
        "outputId": "110bbece-db14-4521-defb-b6b29039dea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "text=\"Mary, don't slap the green witch.\"\n",
        "print([str(token) for token  in nlp(text.lower())]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxftvlF5nB_-",
        "colab_type": "code",
        "outputId": "8af5f180-196a-469c-b458-4808c2bfbecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet=u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:) \"\n",
        "tokenizer=TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet.lower()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rKs3IpJoaYt",
        "colab_type": "text"
      },
      "source": [
        "**Types** are unique tokens present in a corpus. The set of all types in a corpus is its vocabulary or\n",
        "lexicon. Words can be distinguished as **content words** and **stopwords**. Stopwords such as articles and\n",
        "prepositions serve mostly a grammatical purpose, like filler holding the content words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruXCAT6TpKg-",
        "colab_type": "text"
      },
      "source": [
        "**Unigrams, Bigrams, Trigrams, …, N-gram**<br>\n",
        "N­-grams are fixed­length (n) consecutive token sequences occurring in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxoZWnkEoFsh",
        "colab_type": "code",
        "outputId": "e89ca043-4886-464d-b590-b18d8ed68da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def n_grams(text,n):\n",
        "  '''\n",
        "  takes tokens or text, returns a list of n-grams\n",
        "  '''\n",
        "  return [text[i:i+n] for i in range (len(text)-n+1)]\n",
        "cleaned=['mary',',',\"n't\",'slap','green','witch','.']\n",
        "print(n_grams(cleaned,3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWZom1AAtYna",
        "colab_type": "text"
      },
      "source": [
        "**Lemmas and Stems**<br>\n",
        "- **Lemmas**: are root form of words. For example: \"Fly\" can be inflected  into many different words -- flow,flew,flies,flown,flowing,..\"fly\" is the lemma for all of these seemingly different words.<br>\n",
        "Lemmas might be useful to reduce the tokens to their lemmas to *keep the dimensionality of the vector representations low.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVOqFmvqJZl",
        "colab_type": "code",
        "outputId": "14974161-229d-4edf-c758-46c7c20fd3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "nlp=spacy.load('en')\n",
        "doc=nlp(u\"he was running late\")\n",
        "for token in doc:\n",
        "  print('{}-->{}'.format(token,token.lemma_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he-->-PRON-\n",
            "was-->be\n",
            "running-->run\n",
            "late-->late\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aplbb-6Nygdm",
        "colab_type": "text"
      },
      "source": [
        "**Stemming** is the poor­man’s lemmatization.<br> It involves the use of handcrafted rules to strip endings\n",
        "of words to reduce them to a common form called stems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7DdVVjOvkPd",
        "colab_type": "code",
        "outputId": "6abf1beb-9549-480e-d91b-b184127226d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "   \n",
        "ps = PorterStemmer() \n",
        "   \n",
        "#doc=word_tokenize(u\"Programers program with programing languages\")\n",
        "doc=['programers','program','with','programing','languages']   \n",
        "for token in doc:\n",
        "  print('{}-->{}'.format(token,ps.stem(token)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "programers-->program\n",
            "program-->program\n",
            "with-->with\n",
            "programing-->program\n",
            "languages-->languag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhDy87g-0fcV",
        "colab_type": "text"
      },
      "source": [
        "**Categorizing sentences and documents**<br>\n",
        "Problems such as assigning topic\n",
        "labels, predicting sentiment of reviews, filtering spam emails, language identification, and email\n",
        "triaging can be framed as supervised document classification (categorition) problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBQ-5NCJ09KZ",
        "colab_type": "text"
      },
      "source": [
        "**Categorizing words: POS Tagging**<br>\n",
        "We can extend the concept of labeling from documents to individual words or tokens. A common\n",
        "example of categorizing words is part­-of-­speech (POS) tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cBevcVs0EQF",
        "colab_type": "code",
        "outputId": "c35e4986-bb14-4b58-eaec-bfb2cbfdedcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "doc=nlp(u\"Mary slapped the green witch.\")\n",
        "for token in doc:\n",
        "  print(\"{}->{}\".format(token,token.pos_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mary->PROPN\n",
            "slapped->VERB\n",
            "the->DET\n",
            "green->ADJ\n",
            "witch->NOUN\n",
            ".->PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZkVqsIO2808",
        "colab_type": "text"
      },
      "source": [
        "**Categorizing Spans: Chunking and Named Entity Recognition**<br>\n",
        "We often need to label a span of text, that is a contiguous multitoken boundary.<br>\n",
        "For example, \"Marry slapped the green witch.\" -> [NP Mary] [VP slapped] [NP the green witch]. This is called *chunking* or *shallow parsing*.<br>\n",
        "*Shallow parsing* aims to derive higher­order units\n",
        "composed of the grammatical atoms, like nouns, verbs, adjectives, and so on. It is possible to write\n",
        "regular expressions over the part­of­speech tags to approximate shallow parsing if you do not have\n",
        "data to train models for shallow parsing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUpzcVyY1iLg",
        "colab_type": "code",
        "outputId": "9f6faa52-e0f9-4228-936d-7cd1f74f5007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "doc=nlp(u'Marry slapped the green witch.')\n",
        "for chunk in doc.noun_chunks:\n",
        "  print(\"{}->{}\".format(chunk,chunk.label_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Marry->NP\n",
            "the green witch->NP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1l9OH1g5CWR",
        "colab_type": "text"
      },
      "source": [
        "**Structure of sentences**<br>\n",
        "Whereas shallow parsing identifies phrasal units, the task of identifying the relationship between them\n",
        "is called parsing.<br>\n",
        "Parse trees indicate how different grammatical units in a sentence are related hierarchically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VimWRYZR4T9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
